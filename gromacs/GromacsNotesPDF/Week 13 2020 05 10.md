# Week 13 - 2020 05 10

# What I did

This week I mostly investigated different compiles and run time options. I found very little difference in the different compiles (except that the cpu one is about twice as slow as the k40 gpus, and the v100 one is about twice as fast as the k40 one). Most runtime parameters didn't affect the performance in any significant way.

We cannot use the tune_pme program that tunes the number of pme rank, because this simulation doesn't use any electrostatics.

# What I will do

- Grid search parameters + compilers
- investigate single node k40s to try and draw parallels to the dgx that we'll be using.

# Current best results

[Results](https://www.notion.so/61105c5c17bb49f6a8ae953be82cd9fb)

With build script

### V3 → On the V100

- Build script

    ```bash
    module load cmake/3.14.3
    module load gcc/5.5.0
    source /home/mbeukman/intel/2018/parallel_studio_xe_2018.4.057/psxevars.sh
    module load cuda/10.0

    mkdir build
    cd build

    cmake ..  -DREGRESSIONTEST_DOWNLOAD=ON \
     -DGMX_SIMD=AVX_256 \
     -DCMAKE_INSTALL_PREFIX=`pwd`/../install \
     -DCMAKE_C_COMPILER=icc -DCMAKE_CXX_COMPILER=icpc \
     -DGMX_MPI=on -DGMX_GPU=on -DGMX_FFT_LIBRARY=mkl 2>&1 | tee cmake.log
    make -j28 2>&1 | tee make.log
    make check 2>&1 | tee makecheck.log
    make install 2>&1 | tee makeinstall.log
    ```

- Run script

    ```bash
    module load cmake/3.14.3
    module load gcc/5.5.0
    source /home/mbeukman/intel/2018/parallel_studio_xe_2018.4.057/psxevars.sh
    module load cuda/10.0
    source ../install/bin/GMXRC
    which gmx_mpi

    procs=${1:-4}
    args=${2:-""}
    filename=run-$procs-${args// /-/g}".log"

    mpirun -np $procs gmx_mpi mdrun -s lignocellulose-rf.tpr  -gpu_id 0 -nsteps 400 $args  2>&1 | tee logs/$filename
    ```

# Problems

- Nothing seems to change the performance.
- What is nst* mdp options → the messages say that we might want to increase them, but that's in the file we can't access as far as I know.